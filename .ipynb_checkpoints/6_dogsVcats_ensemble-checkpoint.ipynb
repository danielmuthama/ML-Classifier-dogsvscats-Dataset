{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3bSXLX4pkdxs"
   },
   "source": [
    "# Dogs v Cats\n",
    "**End to End Image Classification project using TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IeD6Or9EIHnw"
   },
   "source": [
    "The objective of this project is to develop a model capable of correctly classifying images of Dogs and Cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GCpudobC8dhY"
   },
   "source": [
    "I am going to work on a subset of the original Dogs vs. Cats Dataset (**3000 images sampled from the original dataset of 25000 images**) to demonstrate techniques such as Image Augmentation and Droputs and how these techniques contribute in improving performance. The smaller dataset spares us a lot of computing power. I plan on demonstrating my work on the original dataset which demands GPU/High Computing power soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8bjDaCAEEs-"
   },
   "source": [
    "The Original Dataset with 25,000 training images of dogs and cats can be found [here](https://www.kaggle.com/c/dogs-vs-cats/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/dsm/.local/lib/python3.10/site-packages (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: packaging in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (22.10.26)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/lib/python3/dist-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/lib/python3/dist-packages (from tensorflow) (3.12.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (1.50.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /home/dsm/.local/lib/python3.10/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/dsm/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/dsm/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/dsm/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/dsm/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/dsm/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/dsm/.local/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/dsm/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/dsm/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/dsm/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/dsm/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/dsm/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/dsm/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NCsLrRmuIip"
   },
   "source": [
    "**Get the data** (Smaller Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "S5pqidzJAKPD",
    "outputId": "0cc1118b-026d-437d-a920-5d7047d77ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-05 04:09:27--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.223.80, 172.217.170.176, 172.217.170.208, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.223.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68606236 (65M) [application/zip]\n",
      "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
      "\n",
      " /tmp/cats_and_dogs  80%[===============>    ]  52.60M  1.28MB/s    eta 12s    "
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
    "    -O /tmp/cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TQcJVqBuYi4"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uyi-eUVGTe_j"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BxhrSsN4TC7K"
   },
   "source": [
    "The os library gives us access to the file system, and the zipfile library allows us to unzip the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gatL3NZbvj_p"
   },
   "source": [
    "**Create the respective directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CRLCp-a6vtCN"
   },
   "outputs": [],
   "source": [
    "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "\n",
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBv1aFUMTus-"
   },
   "source": [
    "The contents of the .zip are extracted to the base directory /tmp/cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TU--6Jv0v6PQ"
   },
   "source": [
    "**Model with 4 convolutional layers with 32, 64, 128 and 128 convolutions respectively**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D72OzHazUUK9"
   },
   "source": [
    "We add convolutional layers, and flatten the final result to feed into the densely connected layers.\n",
    "As this is a 2 class classification problem, we use the [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "Aedt-HNewUP3",
    "outputId": "09cc6986-eb07-499d-ea6f-714f8035f3f3"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class and 1 for the other\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "tDtm3juUWpw6",
    "outputId": "7822dc4f-3aa1-47bc-a635-f5161b71d079"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GFnJT_vVWn1M"
   },
   "source": [
    "The \"output shape\" column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions.\n",
    "\n",
    "We use binary_crossentropy as the loss metric as we have 2 target classes (its a binary classification problem)\n",
    "\n",
    "Our optimizer is RMSprop with a learning rate of 0.0001 (We can experiment with this; Adam and Adagrad optimizers would also work well)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "anR2p_UMX9TA",
    "outputId": "b816458e-2cf8-4f30-a953-cb11f9f9a157"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uKCh29ysxB9j"
   },
   "source": [
    "**Rescale the Images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyOO6EZ5YZZ-"
   },
   "source": [
    "The data generators read pictures in our source folders, convert them to float32 tensors, and feed them (with their labels) to our network. We'll have one generator for the training images and one for the validation images.\n",
    "\n",
    "Input data to the neural networks should be normalized to aid in processing. We will preprocess our images by normalizing the pixel values to be in the [0, 1] range (originally all values are in the [0, 255] range).\n",
    "\n",
    "In Keras this can be done via the keras.preprocessing.image.ImageDataGenerator class using the rescale parameter. This ImageDataGenerator class allows you to instantiate generators of augmented image batches (and their labels) via .flow(data, labels) or .flow_from_directory(directory). These generators can then be used with the Keras model methods that accept data generators as inputs: fit_generator, evaluate_generator, and predict_generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6iogIhqTxGeX"
   },
   "outputs": [],
   "source": [
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLhV5NI9xdgD"
   },
   "source": [
    "**Flow images in batches of 20 using train _datagen and test_datagen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "f9D_MjIHx2XD",
    "outputId": "abc41e7c-48a1-4dad-b3fe-4442f2cf6953"
   },
   "outputs": [],
   "source": [
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # This is the source directory for training images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8TfHlDGx8Gz"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UD-vFNgMyim_"
   },
   "source": [
    "Will train for 10 epochs and then plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "m6yLZpibyMXB",
    "outputId": "7944b730-b558-4ee7-9ed7-18021ed7af4a"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
    "      epochs=10,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,  # 1000 images = batch_size * steps\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJFgckBNyhu8"
   },
   "source": [
    "**Accuracy and Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "Bc-Y9ymQz1nq",
    "outputId": "391f6998-8282-480d-b857-0dd569d0e661"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XiGgKIzKz8pW"
   },
   "source": [
    "The graph show that we have a training accuracy of almost **100%**, while the validation accuracy is much lower in the **70%-75%** range.\n",
    "It's a classic case of **Overfitting**\n",
    "\n",
    "Our model does exceptionally well in case of images it has already seen and not so much with unseen images.\n",
    "\n",
    "We can apply a simple yet very effective technique here called **Image Augmentation** where we change the images a bit by rotating them, squashing them, etc.\n",
    "\n",
    "We can implement this using the ImageDataGenerator Class\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "```\n",
    "\n",
    "Some options that are available:\n",
    "\n",
    "\n",
    "\n",
    "*   rotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures.\n",
    "*   width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly     translate pictures vertically or horizontally\n",
    "*   shear_range is for randomly applying shearing transformations\n",
    "*   zoom_range is for randomly zooming inside pictures\n",
    "*   horizontal_flip is for randomly flipping half of the images horizontally. This is relevant when there are no assumptions of horizontal assymmetry (e.g. real-world pictures)\n",
    "*   fill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxLdqu1R26Ym"
   },
   "source": [
    "**Here's the code with added Image Augmentation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wpWxew5t3Elv",
    "outputId": "0cbb21d0-0c71-47b4-a6ad-20adf4bf7357"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
    "    -O /tmp/cats_and_dogs_filtered.zip\n",
    "  \n",
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "\n",
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# This code has changed. Now instead of the ImageGenerator just rescaling\n",
    "# the image, we also rotate and do other operations\n",
    "# Updated to do image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # This is the source directory for training images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
    "      epochs=10,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,  # 1000 images = batch_size * steps\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "yfNfPjpo3Y1-",
    "outputId": "20cda5bd-fb16-4476-b394-3999e517a945"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-40ExjC3obd"
   },
   "source": [
    "As we can see, the accuracy and loss metrics have improved, but we can do better.\n",
    "Let us try implementing **Droput** which would help us with overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2mQ3rtp44lns",
    "outputId": "876e7c2b-8c91-42f2-b32e-21f4019226ad"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
    "    -O /tmp/cats_and_dogs_filtered.zip\n",
    "  \n",
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "\n",
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# This code has changed. Now instead of the ImageGenerator just rescaling\n",
    "# the image, we also rotate and do other operations\n",
    "# Updated to do image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # This is the source directory for training images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,  # 1000 images = batch_size * steps\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "pfQpaqzF4vsq",
    "outputId": "f8850d21-9e3d-4800-df31-4929ac25e291"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1L7ZjJ85ll4"
   },
   "source": [
    "Both the training and validation accuracies are about **82%** with a better loss curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBL71oPF5-Jj"
   },
   "source": [
    "This is a decent result considering that we trained on only 2000 training images versus the original dataset of 22,500 training images.\n",
    "In case of Deep Learning, performance almost always increases when we train on more data.\n",
    "I expect that I will experience a similar result when working with the full dataset next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqWyFYg2FRBh"
   },
   "source": [
    "**Our Model in action**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HG9VmD-AaU8g"
   },
   "source": [
    "Let's check out our model. The following code lets us choose one or more images from the file system. Then the images run through our model and give out results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "colab_type": "code",
    "id": "DRZ_ujVwaRbh",
    "outputId": "166ff272-a7da-45ff-aad7-32c1206c9d65"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    " \n",
    "  # predicting images\n",
    "  path = '/content/' + fn\n",
    "  img = image.load_img(path, target_size=(150, 150))\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "\n",
    "  images = np.vstack([x])\n",
    "  classes = model.predict(images, batch_size=10)\n",
    "  print(classes[0])\n",
    "  if classes[0]>0.5:\n",
    "    print(fn + \" is a dog\")\n",
    "  else:\n",
    "    print(fn + \" is a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tF19ZNcPUYxl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dogsVcats_subset_v2",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
